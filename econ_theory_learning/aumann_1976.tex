
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}{**DATE**}
\phantomsection \addcontentsline{toc}{section}{Aumann (1976)}
\chno{1}{\textit{Agreeing to Disagree},  Aumann (1976) }{Annie Liang}{Zach Schutzman}{January 10, 2018}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.


\section*{Overview}
\begin{enumerate}
	\item defined ``common knowledge''
	\item introduced the partitional model (called the Aumann model now)
	\item showed that agents cannot ``agree to disagree'' under a common prior
\end{enumerate}

Idea: If there is a common prior and common knowledge of posterior beliefs (I know that you know that I know... and so on, forever), then the posteriors are identical.

\section*{Framework}

Let $(\Omega, \mathcal{B},p)$ be a probability space, with $(\Omega, B)$ the state space and $p$ a common prior.  Denote by $\mathcal{P}_i$ the partition of Player $i$.  At the state $\omega\in\Omega$, Player $i$ learns $P_i(\omega)$, that is, the chunk of her partition which contains $\omega$.  Note that we are implicitly assuming that a player cannot believe that the true state of the world is $\omega\in A$ if in fact $\omega\notin A$.


We can illustrate this with an example:

 $\Omega=\{1,2,3,4,5,6\}$\\$\mathcal{P}_1=\{\{1,2,3\},\{4,5\},\{6\}\}$\\$\mathcal{P}_2=\{\{1,2\},\{3,4\},\{5\},\{6\}\}$
 
 The \textbf{join} $\mathcal{P}_1\lor\mathcal{P}_2$ is the coarsest common refinement of $\mathcal{P}_1$ and $\mathcal{P}_2$.
 
 $\rightarrow$ in our example, $\mathcal{P}_1\lor\mathcal{P}_2=\{\{1,2\},\{3\},\{4\},\{5\},\{6\}\}$
 
 We can think of the join as `what players can get by pooling their knowledge'.
 
 The \textbf{meet} $\mathcal{P}_1\land\mathcal{P}_2$ is the finest common coarsening of $\mathcal{P}_1$ and $\mathcal{P}_2$.
 
  $\rightarrow$ in our example, $\mathcal{P}_1\land\mathcal{P}_2=\{\{1,2,3,4,5\},\{6\}\}$
  
  Given a state $\omega\in \Omega$, an event $E$ is \textbf{common knowledge at $\boldsymbol{\omega}$} if $E$ includes the member of the meet which includes $\omega$.
  
  $\rightarrow$ The set $\{1,2,3,4,5\}$ is common knowledge at $\omega=3$.
  
  Observe that $\Omega$ itself is always common knowledge.
  
  \section*{Result}
  Fix an event $A$.  Let $q_i$ be the posterior probability of $A$ given Player $i$'s information.  That is,
  $$q_i=\frac{p(A\cap P_i(\omega))}{p(P_i(\omega))}$$
  
 \underline{ If it is common knowledge at $\omega$ that $q_1=a$ and $q_2=b$ then $a=b$.}
 
 \begin{proof}
 	Let $P$ be the member of the meet containing $\omega$.  Write $P=\bigcup\limits_j P_j$ where the $P_j$ are disjoint elements of $P_1$.
 	
 	Since $q_1=a$ is common knowledge at $\omega$, it must be that $q_1=a$ at each partition element $P_j$.  Thus, for all $j$,
 	$$a=p(A\cap P_j)/p(P_j)$$
 	$$ap(P_j)=p(A\cap P_j)$$
 	$$a\sum\limits_j p(P_j) = \sum\limits_j p(A\cap P_j)$$
 	$$ap(P)=p(A\cap P)$$
 	
 By doing the same thing for Player 2, we can get an expression that says $bp(P)=p(A\cap P)$, so $a=b$.
 \end{proof}
  
  
  $\star$ Note that knowledge of the posterior alone is not sufficient.  Consider the following example:\\
  $\Omega=\{1,2,3,4\}$\\$\mathcal{P}_1=\{\{1,2\},\{3,4\}\}$\\$\mathcal{P}_2=\{\{1,2,3\},\{4\}\}$\\$\omega=2$, $A=\{1,4\}$, uniform prior\\
  
  Player 1's posterior at $A$ is $1/2$, Player 2's is $1/3$.  Player 2 knows that Player 1's posterior is $1/2$, but not \textit{why}, as Player 2 doesn't know which of the two chunks she has been told contains $\omega$.



















